# Pointer Network for Audio Editing

A sequence-to-sequence model that learns to edit audio by predicting frame pointer sequences. Given a raw audio mel spectrogram, the model outputs a sequence of pointers indicating which raw frames to use (and in what order) to reconstruct the edited version.

## Overview

Unlike traditional audio editing approaches that try to directly generate mel spectrograms, the pointer network approach:
- **Preserves audio quality** by copying frames from the original
- **Learns editing patterns** (cuts, loops, reordering) from examples
- **Handles variable-length outputs** with STOP token
- **Supports diverse edit styles** via VAE latent space

## Architecture

```
Raw Mel (B, n_mels, T_raw)
          │
          ▼
┌─────────────────────────────┐
│    Multi-Scale Encoder      │
│  - Frame level (1x)         │
│  - Beat level (43x)         │
│  - Bar level (172x)         │
├─────────────────────────────┤
│  Music-Aware Positional     │
│  Encoding (beat/bar/phrase) │
├─────────────────────────────┤
│    Edit Style VAE           │
│  (latent space for style)   │
└─────────────────────────────┘
          │
          ▼
┌─────────────────────────────┐
│    Hierarchical Decoder     │
│  - Sparse attention (O(n))  │
│  - KV caching for inference │
│  - Cross-attention to raw   │
├─────────────────────────────┤
│    Output Heads             │
│  - Pointer logits           │
│  - Stop prediction          │
│  - Structure prediction     │
└─────────────────────────────┘
          │
          ▼
Pointer Sequence (B, T_edit)
```

## Features

### Core
- **Pointer-based output**: Model points to frames rather than generating them
- **Hierarchical pointers**: Coarse-to-fine prediction (bar → beat → frame)
- **STOP token**: Handles variable-length output sequences
- **Length prediction**: Predicts output length for planning

### Efficiency
- **Sparse attention**: Local window + strided global (O(n) not O(n²))
- **KV caching**: 2-3x faster autoregressive inference
- **Chunked processing**: Handle very long sequences
- **Multi-scale encoding**: Process at frame/beat/bar levels

### Musical Awareness
- **Music-aware positional encoding**: Beat, bar, phrase structure
- **Hierarchical attention**: Bar → Beat → Frame cascade
- **Structure prediction**: Auxiliary task for cut/loop detection

### Diversity
- **Edit Style VAE**: Latent space enables diverse outputs
- **Duration conditioning**: Control output length

## Installation

```bash
cd F:\editorbot
pip install torch librosa numpy tqdm
```

## Usage

### Training

```bash
# Generate pointer sequences from aligned audio pairs
python _generate_pointer_sequences.py

# Train the pointer network
python -m pointer_network.trainers.pointer_trainer \
    --cache-dir training_data/super_editor_cache \
    --pointer-dir training_data/pointer_sequences \
    --save-dir models/pointer_network \
    --epochs 100 \
    --batch-size 4
```

### Inference

```python
from pointer_network import PointerNetwork, TrainConfig
import torch

# Load model
config = TrainConfig()
model = PointerNetwork().cuda()
model.load_state_dict(torch.load('models/pointer_network/best.pt')['model_state_dict'])
model.eval()

# Inference
with torch.no_grad():
    pointers, stop_probs = model.inference(raw_mel.unsqueeze(0))
```

## Configuration

### Model Config (`PointerNetworkConfig`)

| Parameter | Default | Description |
|-----------|---------|-------------|
| `n_mels` | 128 | Mel spectrogram bins |
| `d_model` | 256 | Transformer hidden dimension |
| `n_heads` | 8 | Attention heads |
| `n_encoder_layers` | 4 | Encoder transformer layers |
| `n_decoder_layers` | 4 | Decoder transformer layers |
| `chunk_size` | 512 | Frames per chunk |
| `dropout` | 0.1 | Dropout rate |

### Training Config (`TrainConfig`)

| Parameter | Default | Description |
|-----------|---------|-------------|
| `batch_size` | 4 | Batch size |
| `epochs` | 100 | Training epochs |
| `learning_rate` | 1e-4 | Initial learning rate |
| `weight_decay` | 0.01 | AdamW weight decay |

## Loss Functions

| Loss | Weight | Purpose |
|------|--------|---------|
| Pointer CE | 1.0 | Main pointer prediction |
| Length MSE | 0.1 | Output length prediction |
| Stop BCE | 0.5 | Stop token prediction |
| KL Divergence | 0.01 | VAE regularization |
| Smoothness | 0.01 | Encourage smooth sequences |

## Data Format

### Input
- **Raw mel**: `(B, n_mels, T_raw)` - Mel spectrogram of raw audio
- **Target pointers**: `(B, T_edit)` - Frame indices into raw mel

### Pointer Sequences
Generated by `_generate_pointer_sequences.py`:
```
training_data/pointer_sequences/
├── {name}_pointers.npy    # (T_edit,) int32 array of frame indices
├── {name}_info.json       # Metadata (paths, frame counts, etc.)
└── {name}_alignment.png   # Visualization of alignment
```

### Special Tokens
- `STOP_TOKEN = -2`: End of sequence
- `PAD_TOKEN = -1`: Padding

## Project Structure

```
pointer_network/
├── __init__.py              # Package exports
├── config.py                # Configuration dataclasses
├── README.md                # This file
├── models/
│   ├── __init__.py
│   ├── pointer_network.py   # Main consolidated model
│   ├── encoder.py           # Legacy encoder components
│   └── decoder.py           # Legacy decoder components
├── data/
│   ├── __init__.py
│   └── dataset.py           # PointerDataset, collate_fn
├── losses/
│   ├── __init__.py
│   └── pointer_losses.py    # All loss functions
└── trainers/
    ├── __init__.py
    └── pointer_trainer.py   # Training loop
```

## Training Tips

1. **Start small**: Use `chunk_size=256` initially, increase after validation
2. **Monitor smoothness**: High smoothness loss → too many spurious jumps
3. **Check alignment ratio**: Should be >95% for good training data
4. **Use mixed precision**: Add `torch.cuda.amp` for 2x speedup
5. **Gradient clipping**: Keep at 1.0 to prevent explosions

## Metrics

| Metric | Target | Description |
|--------|--------|-------------|
| Accuracy | >80% | Exact pointer match |
| Within-5 | >95% | Pointer within 5 frames |
| Length MAE | <50 | Mean absolute length error |
| Stop Precision | >90% | Correct stop predictions |

## Key Differences from Other Approaches

| Approach | Pros | Cons |
|----------|------|------|
| **Pointer Network** | Preserves quality, learns patterns | Can't generate new content |
| Mel-to-Mel | End-to-end | Quality degradation |
| Edit Labels + Reconstruction | Interpretable | Two-stage complexity |
| RL (PPO) | Optimizes directly | Sample inefficient |

## Hierarchical Pointers (NEW)

The model now supports **coarse-to-fine hierarchical pointer prediction**:

```
Bar Level:   [Bar 0] [Bar 1] [Bar 2] ...     (n_bars = T_frames / 172)
              ↓
Beat Level:  [Beat 0-3 within bar] ...       (n_beats = T_frames / 43)
              ↓
Frame Level: [Frames 0-42 within beat] ...   (T_frames)
```

### How It Works

1. **Training**: Three parallel losses computed from frame-level targets
   - Bar loss: `target_bar = frame_pointer // frames_per_bar`
   - Beat loss: `target_beat = frame_pointer // frames_per_beat`
   - Frame loss: Original frame pointer prediction

2. **Generation**: Two modes available
   - `generate()`: Standard frame-level prediction
   - `generate_hierarchical(coarse_to_fine=True)`:
     - Sample bar first
     - Sample beat constrained to selected bar
     - Sample frame constrained to selected beat

### Benefits

- **More structured edits**: Respects musical boundaries (bars, beats)
- **Better long-range coherence**: Coarse predictions guide fine details
- **Reduced error accumulation**: Hierarchical constraint prevents drift
- **Interpretable**: Can inspect bar/beat decisions separately

### Usage

```python
# Standard generation
result = model.generate(raw_mel)

# Hierarchical coarse-to-fine generation
result = model.generate_hierarchical(raw_mel, coarse_to_fine=True)
print(result['bar_indices'])   # Which bars were selected
print(result['beat_indices'])  # Which beats within bars
print(result['pointers'])      # Final frame pointers
```

## Future Improvements

- [x] Hierarchical pointers (bar → beat → frame) ✅ IMPLEMENTED
- [ ] Edit operations format (COPY, LOOP, etc.)
- [ ] Style conditioning from reference tracks
- [ ] RL fine-tuning with audio quality rewards
- [ ] Multi-track awareness for stem coherence
