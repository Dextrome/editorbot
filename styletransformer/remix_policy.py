"""
Remix Policy Network - Learns to make remix decisions that match a target style.

The policy network takes:
- Source audio features (from raw recording)
- Target style embedding (from reference song)

And outputs:
- Phrase selection probabilities
- Transition parameters (crossfade length, energy matching)
- Ordering decisions
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from dataclasses import dataclass
from typing import List, Tuple, Optional, Dict
import logging

logger = logging.getLogger(__name__)


@dataclass
class RemixAction:
    """A single remix decision."""
    phrase_index: int           # Which phrase to use next
    crossfade_duration: float   # Crossfade length in seconds
    energy_adjust: float        # Energy adjustment factor (0.5 - 2.0)
    pitch_shift: float          # Pitch shift in semitones (-2 to 2)


@dataclass
class RemixPlan:
    """Complete remix plan generated by policy."""
    actions: List[RemixAction]
    expected_duration: float
    style_match_score: float    # Predicted similarity to target


class PhraseEncoder(nn.Module):
    """Encodes a single phrase's features into a vector."""
    
    def __init__(self, feature_dim: int = 64, hidden_dim: int = 128):
        super().__init__()
        
        # Input features per phrase:
        # - energy, brightness, onset_density, duration (4)
        # - chroma (12)
        # - start/end energy (2)
        # - quality score (1)
        # Total: 19
        
        self.mlp = nn.Sequential(
            nn.Linear(19, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, feature_dim)
        )
    
    def forward(self, phrase_features: torch.Tensor) -> torch.Tensor:
        """
        Args:
            phrase_features: (B, N, 19) where N is number of phrases
        Returns:
            encoded: (B, N, feature_dim)
        """
        return self.mlp(phrase_features)


class RemixPolicyNet(nn.Module):
    """
    Policy network that generates remix decisions.
    
    Uses attention mechanism to:
    1. Attend to source phrases based on target style
    2. Generate sequence of remix actions
    """
    
    def __init__(
        self,
        style_dim: int = 256,
        phrase_dim: int = 64,
        hidden_dim: int = 256,
        num_heads: int = 8,
        num_layers: int = 4,
        max_phrases: int = 100
    ):
        super().__init__()
        
        self.style_dim = style_dim
        self.phrase_dim = phrase_dim
        self.hidden_dim = hidden_dim
        self.max_phrases = max_phrases
        
        # Phrase encoder
        self.phrase_encoder = PhraseEncoder(phrase_dim)
        
        # Style projection (to match phrase dimension)
        self.style_proj = nn.Linear(style_dim, phrase_dim)
        
        # Cross-attention: style attends to phrases
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=phrase_dim,
            num_heads=num_heads,
            batch_first=True
        )
        
        # Autoregressive decoder for generating sequence
        self.decoder = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(
                d_model=hidden_dim,
                nhead=num_heads,
                dim_feedforward=hidden_dim * 4,
                dropout=0.1,
                batch_first=True
            ),
            num_layers=num_layers
        )
        
        # Action prediction heads
        # Phrase selection (softmax over available phrases)
        self.phrase_head = nn.Linear(hidden_dim, max_phrases)
        
        # Continuous action parameters
        self.param_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 3)  # crossfade, energy_adjust, pitch_shift
        )
        
        # Stop prediction (when to end the remix)
        self.stop_head = nn.Linear(hidden_dim, 1)
        
        # Memory/state projection
        self.state_proj = nn.Linear(phrase_dim + phrase_dim, hidden_dim)
        
        # Position encoding for decoder
        self.pos_encoding = nn.Parameter(torch.randn(1, max_phrases, hidden_dim) * 0.02)
    
    def forward(
        self,
        phrase_features: torch.Tensor,   # (B, N, 19) source phrase features
        style_embedding: torch.Tensor,   # (B, style_dim) target style
        num_phrases: int = 10,           # Number of phrases to generate
        phrase_mask: Optional[torch.Tensor] = None  # (B, N) valid phrase mask
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Generate remix actions.
        
        Returns:
            phrase_logits: (B, num_phrases, max_phrases) selection probabilities
            params: (B, num_phrases, 3) continuous parameters
            stop_logits: (B, num_phrases, 1) stop probabilities
        """
        B, N, _ = phrase_features.shape
        
        # Encode phrases
        phrase_enc = self.phrase_encoder(phrase_features)  # (B, N, phrase_dim)
        
        # Project style
        style_proj = self.style_proj(style_embedding)  # (B, phrase_dim)
        style_query = style_proj.unsqueeze(1)  # (B, 1, phrase_dim)
        
        # Cross-attention: what phrases match the style?
        style_context, _ = self.cross_attention(
            style_query, phrase_enc, phrase_enc,
            key_padding_mask=~phrase_mask if phrase_mask is not None else None
        )  # (B, 1, phrase_dim)
        
        # Initialize decoder state
        # Combine style and context
        init_state = self.state_proj(
            torch.cat([style_proj, style_context.squeeze(1)], dim=-1)
        )  # (B, hidden_dim)
        
        # Decoder memory (phrase encodings projected to hidden_dim)
        memory = F.linear(phrase_enc, torch.eye(self.phrase_dim, self.hidden_dim, device=phrase_enc.device).T)
        
        # Generate sequence autoregressively
        phrase_logits_list = []
        params_list = []
        stop_logits_list = []
        
        # Start token
        decoder_input = init_state.unsqueeze(1)  # (B, 1, hidden_dim)
        
        for t in range(num_phrases):
            # Add position encoding
            pos_enc = self.pos_encoding[:, :decoder_input.size(1), :]
            decoder_in = decoder_input + pos_enc
            
            # Decode
            decoded = self.decoder(decoder_in, memory)  # (B, t+1, hidden_dim)
            last_hidden = decoded[:, -1, :]  # (B, hidden_dim)
            
            # Predict actions
            phrase_logits = self.phrase_head(last_hidden)  # (B, max_phrases)
            params = self.param_head(last_hidden)  # (B, 3)
            stop_logits = self.stop_head(last_hidden)  # (B, 1)
            
            phrase_logits_list.append(phrase_logits)
            params_list.append(params)
            stop_logits_list.append(stop_logits)
            
            # Update decoder input for next step
            # Use predicted phrase features as next input
            if t < num_phrases - 1:
                # Soft attention over phrases based on logits
                if phrase_mask is not None:
                    masked_logits = phrase_logits.clone()
                    masked_logits[:, :N][~phrase_mask] = float('-inf')
                else:
                    masked_logits = phrase_logits[:, :N]
                
                attn_weights = F.softmax(masked_logits, dim=-1)  # (B, N)
                selected_phrase = torch.bmm(attn_weights.unsqueeze(1), phrase_enc)  # (B, 1, phrase_dim)
                
                # Project and append
                next_input = F.linear(
                    selected_phrase, 
                    torch.eye(self.phrase_dim, self.hidden_dim, device=selected_phrase.device).T
                )
                decoder_input = torch.cat([decoder_input, next_input], dim=1)
        
        # Stack outputs
        phrase_logits = torch.stack(phrase_logits_list, dim=1)  # (B, num_phrases, max_phrases)
        params = torch.stack(params_list, dim=1)  # (B, num_phrases, 3)
        stop_logits = torch.stack(stop_logits_list, dim=1)  # (B, num_phrases, 1)
        
        return phrase_logits, params, stop_logits
    
    def sample(
        self,
        phrase_features: torch.Tensor,
        style_embedding: torch.Tensor,
        max_phrases: int = 20,
        temperature: float = 1.0,
        phrase_mask: Optional[torch.Tensor] = None
    ) -> List[RemixAction]:
        """
        Sample a remix plan from the policy.
        
        Args:
            phrase_features: (1, N, 19) source phrases
            style_embedding: (1, style_dim) target style
            max_phrases: Maximum phrases to generate
            temperature: Sampling temperature
            phrase_mask: (1, N) valid phrase mask
            
        Returns:
            List of RemixAction
        """
        self.eval()
        
        with torch.no_grad():
            phrase_logits, params, stop_logits = self.forward(
                phrase_features, style_embedding, max_phrases, phrase_mask
            )
            
            actions = []
            N = phrase_features.size(1)
            
            for t in range(max_phrases):
                # Check stop condition
                if torch.sigmoid(stop_logits[0, t, 0]).item() > 0.5:
                    break
                
                # Sample phrase
                logits = phrase_logits[0, t, :N] / temperature
                if phrase_mask is not None:
                    logits[~phrase_mask[0]] = float('-inf')
                
                probs = F.softmax(logits, dim=-1)
                phrase_idx = torch.multinomial(probs, 1).item()
                
                # Get parameters
                p = params[0, t]
                crossfade = torch.sigmoid(p[0]).item() * 2.0  # 0-2 seconds
                energy_adj = 0.5 + torch.sigmoid(p[1]).item() * 1.5  # 0.5-2.0
                pitch_shift = torch.tanh(p[2]).item() * 2.0  # -2 to +2 semitones
                
                actions.append(RemixAction(
                    phrase_index=phrase_idx,
                    crossfade_duration=crossfade,
                    energy_adjust=energy_adj,
                    pitch_shift=pitch_shift
                ))
        
        return actions


class RemixPolicy:
    """High-level interface for remix policy."""
    
    def __init__(
        self,
        model_path: Optional[str] = None,
        device: str = "cuda" if torch.cuda.is_available() else "cpu",
        style_dim: int = 256,
        hidden_dim: int = 512
    ):
        self.device = device
        self.model = RemixPolicyNet(style_dim=style_dim, hidden_dim=hidden_dim)
        self.model.to(device)
        
        if model_path:
            self.load(model_path)
    
    def load(self, path: str):
        """Load trained model."""
        state_dict = torch.load(path, map_location=self.device, weights_only=True)
        self.model.load_state_dict(state_dict)
        logger.info(f"Loaded remix policy from {path}")
    
    def save(self, path: str):
        """Save model."""
        torch.save(self.model.state_dict(), path)
        logger.info(f"Saved remix policy to {path}")
    
    def generate_plan(
        self,
        source_phrases: List[Dict],
        style_embedding: np.ndarray,
        target_duration: float = 180.0,
        temperature: float = 0.8
    ) -> RemixPlan:
        """
        Generate a remix plan given source phrases and target style.
        
        Args:
            source_phrases: List of phrase dicts with features
            style_embedding: Target style embedding from StyleEncoder
            target_duration: Target duration in seconds
            temperature: Sampling temperature
            
        Returns:
            RemixPlan with actions
        """
        # Convert phrases to tensor
        phrase_features = self._phrases_to_tensor(source_phrases)
        style_tensor = torch.FloatTensor(style_embedding).unsqueeze(0).to(self.device)
        
        # Estimate number of phrases needed (with some margin)
        avg_duration = np.mean([p.get('duration', 8.0) for p in source_phrases])
        num_phrases = int(target_duration / avg_duration) + 5  # Extra margin
        
        # Sample actions with duration-based stopping
        actions = self._sample_with_duration_target(
            phrase_features,
            style_tensor,
            source_phrases,
            target_duration,
            max_phrases=num_phrases,
            temperature=temperature
        )
        
        # Calculate expected duration
        expected_duration = sum(
            source_phrases[a.phrase_index].get('duration', 8.0)
            for a in actions
        )
        
        return RemixPlan(
            actions=actions,
            expected_duration=expected_duration,
            style_match_score=0.0  # Will be computed by discriminator
        )
    
    def _sample_with_duration_target(
        self,
        phrase_features: torch.Tensor,
        style_embedding: torch.Tensor,
        source_phrases: List[Dict],
        target_duration: float,
        max_phrases: int = 20,
        temperature: float = 1.0
    ) -> List[RemixAction]:
        """
        Sample actions until we reach the target duration.
        Uses duration-based stopping instead of learned stop signal.
        """
        self.model.eval()
        
        # Compute style-phrase similarity scores for better selection
        style_scores = self._compute_phrase_style_scores(phrase_features, style_embedding)
        
        with torch.no_grad():
            phrase_logits, params, stop_logits = self.model.forward(
                phrase_features, style_embedding, max_phrases, None
            )
            
            actions = []
            current_duration = 0.0
            N = phrase_features.size(1)
            used_phrases = set()  # Avoid repeating phrases too much
            
            for t in range(max_phrases):
                # Stop if we've reached target duration
                if current_duration >= target_duration:
                    break
                
                # Sample phrase (with preference for unused phrases)
                logits = phrase_logits[0, t, :N] / temperature
                
                # Add style-matching bonus (scale by 2.0 for stronger effect)
                logits = logits + style_scores * 2.0
                
                # Penalize recently used phrases
                for idx in used_phrases:
                    if idx < N:
                        logits[idx] -= 2.0  # Soft penalty, not hard exclusion
                
                probs = F.softmax(logits, dim=-1)
                phrase_idx = torch.multinomial(probs, 1).item()
                
                # Get parameters
                p = params[0, t]
                crossfade = torch.sigmoid(p[0]).item() * 2.0  # 0-2 seconds
                energy_adj = 0.5 + torch.sigmoid(p[1]).item() * 1.5  # 0.5-2.0
                pitch_shift = torch.tanh(p[2]).item() * 2.0  # -2 to +2 semitones
                
                actions.append(RemixAction(
                    phrase_index=phrase_idx,
                    crossfade_duration=crossfade,
                    energy_adjust=energy_adj,
                    pitch_shift=pitch_shift
                ))
                
                # Update duration and tracking
                phrase_duration = source_phrases[phrase_idx].get('duration', 8.0)
                current_duration += phrase_duration
                used_phrases.add(phrase_idx)
                
                # Clear used set periodically to allow phrase reuse in longer songs
                if len(used_phrases) > len(source_phrases) * 0.7:
                    used_phrases.clear()
        
        return actions
    
    def _phrases_to_tensor(self, phrases: List[Dict]) -> torch.Tensor:
        """Convert phrase list to feature tensor."""
        features = []
        
        for p in phrases:
            f = np.zeros(19)
            f[0] = p.get('energy', 0.5)
            f[1] = p.get('brightness', 0.5)
            f[2] = p.get('onset_density', 5.0) / 20.0  # Normalize
            f[3] = p.get('duration', 8.0) / 30.0  # Normalize
            f[4] = p.get('start_energy', 0.5)
            f[5] = p.get('end_energy', 0.5)
            f[6] = p.get('quality_score', 0.5)
            
            # Chroma (12 values)
            chroma = p.get('avg_chroma', np.zeros(12))
            if chroma is not None:
                f[7:19] = chroma[:12]
            
            features.append(f)
        
        tensor = torch.FloatTensor(features).unsqueeze(0).to(self.device)
        return tensor
    
    def _compute_phrase_style_scores(
        self,
        phrase_features: torch.Tensor,
        style_embedding: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute how well each phrase matches the target style.
        Uses the model's cross-attention mechanism to score phrases.
        
        Returns:
            Tensor of shape (N,) with style-match scores for each phrase
        """
        with torch.no_grad():
            # Encode phrases
            phrase_enc = self.model.phrase_encoder(phrase_features)  # (1, N, phrase_dim)
            
            # Project style
            style_proj = self.model.style_proj(style_embedding)  # (1, phrase_dim)
            
            # Compute attention scores between style and each phrase
            # This gives us a measure of how much each phrase "matches" the style
            style_query = style_proj.unsqueeze(1)  # (1, 1, phrase_dim)
            
            # Dot product attention scores
            scores = torch.bmm(style_query, phrase_enc.transpose(1, 2))  # (1, 1, N)
            scores = scores.squeeze(0).squeeze(0)  # (N,)
            
            # Normalize to reasonable range
            scores = scores / (self.model.phrase_dim ** 0.5)
            
            return scores
