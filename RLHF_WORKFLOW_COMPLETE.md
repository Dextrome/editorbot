# Complete RLHF Workflow: From Training to Human Feedback

## The Full Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: INITIAL TRAINING (Completed âœ“)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  Step 1: Generate dense reward signals                           â”‚
â”‚  â”œâ”€ Tempo consistency: beats align throughout edit             â”‚
â”‚  â”œâ”€ Energy flow: smooth dynamics without jarring drops         â”‚
â”‚  â”œâ”€ Phrase completeness: respect musical structure             â”‚
â”‚  â””â”€ Transition quality: beats align well at boundaries          â”‚
â”‚                                                                   â”‚
â”‚  Step 2: Collect synthetic preference pairs                      â”‚
â”‚  â””â”€ Bradley-Terry training: model learns to score edits         â”‚
â”‚     â†’ Result: reward_model_v8 (2.63M parameters)                â”‚
â”‚                                                                   â”‚
â”‚  Step 3: Train policy with learned rewards                       â”‚
â”‚  â””â”€ PPO agent learns: 80% learned rewards + 20% dense           â”‚
â”‚     â†’ Result: policy_final.pt (889 state dims, 9 actions)       â”‚
â”‚                                                                   â”‚
â”‚  Status: âœ“ COMPLETE (200 episodes trained)                      â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: HUMAN FEEDBACK COLLECTION (Your Turn Now!)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  Step 1: Generate Candidate Edits (~1 hour)                     â”‚
â”‚  â”œâ”€ Use trained policy with different "temperatures"            â”‚
â”‚  â”œâ”€ Temperature 0.1 = conservative (keeps more beats)           â”‚
â”‚  â”œâ”€ Temperature 0.9 = aggressive (cuts more, tighter)           â”‚
â”‚  â””â”€ For each song: 5 versions with different styles             â”‚
â”‚                                                                   â”‚
â”‚  Command:                                                        â”‚
â”‚  $ python generate_eval_candidates.py \                          â”‚
â”‚      --songs_dir data/test_songs \                              â”‚
â”‚      --n_songs 10 \                                             â”‚
â”‚      --candidates_per_song 5                                    â”‚
â”‚                                                                   â”‚
â”‚  Output: eval_outputs/evaluation_manifest.json                   â”‚
â”‚  â”‚       + 50 audio files (10 songs Ã— 5 versions each)           â”‚
â”‚  â”‚       + Pairwise comparison tasks ready for humans            â”‚
â”‚                                                                   â”‚
â”‚  Step 2: Collect Human Preferences (~2-5 hours)                 â”‚
â”‚  â”œâ”€ Listen to edit pairs                                        â”‚
â”‚  â”œâ”€ Choose: Which is better? A / B / Tie                        â”‚
â”‚  â”œâ”€ Rate strength: Slightly / Moderately / Significantly        â”‚
â”‚  â””â”€ Optional: Add notes explaining preference                   â”‚
â”‚                                                                   â”‚
â”‚  Example Annotation:                                             â”‚
â”‚  {                                                              â”‚
â”‚    "song_id": "song_001",                                       â”‚
â”‚    "edit_a_id": "temp_0.1",                                     â”‚
â”‚    "edit_b_id": "temp_0.5",                                     â”‚
â”‚    "preference": "a",                                           â”‚
â”‚    "strength": 2,                                               â”‚
â”‚    "reasoning": "Tighter without losing vocal hook"            â”‚
â”‚  }                                                              â”‚
â”‚                                                                   â”‚
â”‚  Format: JSON (see FEEDBACK_DATA_FORMAT.md)                      â”‚
â”‚  File: feedback/preferences.json                                 â”‚
â”‚                                                                   â”‚
â”‚  Step 3: Train Reward Model on Feedback (~30 minutes)            â”‚
â”‚  â”œâ”€ Load v8 (pre-trained on synthetic data)                    â”‚
â”‚  â”œâ”€ Fine-tune on human preferences                              â”‚
â”‚  â”œâ”€ Bradley-Terry loss: reward(A) - reward(B) â‰ˆ preference      â”‚
â”‚  â””â”€ Save: reward_model_v9_feedback_final.pt                     â”‚
â”‚                                                                   â”‚
â”‚  Command:                                                        â”‚
â”‚  $ python train_from_feedback.py \                               â”‚
â”‚      --feedback feedback/preferences.json \                      â”‚
â”‚      --pretrained models/reward_model_v8_long/... \              â”‚
â”‚      --epochs 20                                                â”‚
â”‚                                                                   â”‚
â”‚  Output: reward_model_v9_feedback_final.pt                       â”‚
â”‚                                                                   â”‚
â”‚  Status: Ready to start (you are here!)                         â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: POLICY REFINEMENT (Next Step!)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  Step 1: Fine-tune Policy with Human Preferences (~1-2 hours)   â”‚
â”‚  â”œâ”€ Load policy_final.pt (trained on v8)                        â”‚
â”‚  â”œâ”€ Use reward_model_v9 (trained on human feedback)             â”‚
â”‚  â”œâ”€ Continue PPO training: now chasing human preferences        â”‚
â”‚  â””â”€ Result: policy_v15 (even better edits!)                     â”‚
â”‚                                                                   â”‚
â”‚  Command:                                                        â”‚
â”‚  $ python train_rlhf_stable.py \                                 â”‚
â”‚      --episodes 500 \                                           â”‚
â”‚      --reward_model models/reward_model_v9_feedback_final.pt    â”‚
â”‚                                                                   â”‚
â”‚  Output: models/policy_final.pt (updated)                        â”‚
â”‚                                                                   â”‚
â”‚  Step 2: Evaluate Improvements                                  â”‚
â”‚  â”œâ”€ Generate new edits with policy_v15                          â”‚
â”‚  â”œâ”€ Have humans rate: "How much better?"                        â”‚
â”‚  â””â”€ Measure win rate: target >60% improvement                   â”‚
â”‚                                                                   â”‚
â”‚  Status: Ready after feedback collection                        â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 4: ITERATION (Optional - Repeat for Excellence)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  Cycle 2: More Human Feedback                                   â”‚
â”‚  â””â”€ Generate edits with policy_v15                              â”‚
â”‚     Collect 200+ more preference pairs                          â”‚
â”‚     Train reward_model_v10                                      â”‚
â”‚     Fine-tune policy_v16                                        â”‚
â”‚     â†’ Further improvement!                                      â”‚
â”‚                                                                   â”‚
â”‚  Cycle 3: Active Learning                                       â”‚
â”‚  â””â”€ Identify "hard cases" where models disagree                â”‚
â”‚     Collect feedback on disagreements                           â”‚
â”‚     Train reward_model_v11                                      â”‚
â”‚     â†’ Resolves ambiguities!                                     â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Quick Start Guide

### You Are Here (Human Feedback Phase)

#### Timeline: 2-5 hours depending on effort

**Task 1: Generate Candidates (1 hour)**
```bash
python generate_eval_candidates.py \
  --songs_dir data/test_songs \
  --output_dir eval_outputs \
  --n_songs 10 \
  --candidates_per_song 5
```

Output: `eval_outputs/`
- `evaluation_manifest.json` - List all tasks
- `song_001/` - Audio files for song 1
- `song_002/` - Audio files for song 2
- etc.

**Task 2: Collect Feedback (2-4 hours)**

1. Open `eval_outputs/evaluation_manifest.json`
2. For each song, get the candidate audio files
3. Listen to each pair
4. For each comparison, choose:
   - **Preference**: Which is better? (A / B / Tie)
   - **Strength**: How much? (1=Slight, 2=Moderate, 3=Strong)
   - **Notes**: Why? (optional)

Save as `feedback/preferences.json`

**Task 3: Train Reward Model (30 minutes)**
```bash
python train_from_feedback.py \
  --feedback feedback/preferences.json \
  --epochs 20
```

Output:
- `models/reward_model_v9_feedback_final.pt`
- Training history with metrics

**Task 4: Fine-tune Policy (1-2 hours)**
```bash
python train_rlhf_stable.py \
  --episodes 500 \
  --reward_model models/reward_model_v9_feedback_final.pt
```

Output:
- `models/policy_final.pt` (updated with human preferences)
- Training logs showing improvements

---

## Key Concepts

### Temperature (Controls Edit Aggressiveness)

```
Temperature 0.1  â†’  Very Conservative  â†’  Keeps most beats  â†’  ~180 sec
Temperature 0.3  â†’  Moderately Keep    â†’  Removes few beats  â†’  ~168 sec
Temperature 0.5  â†’  Balanced           â†’  Mixed strategy     â†’  ~150 sec
Temperature 0.7  â†’  Moderately Cut     â†’  Removes many beats â†’  ~132 sec
Temperature 0.9  â†’  Very Aggressive    â†’  Only keeps best    â†’  ~90 sec
```

The policy generates different edits based on temperature. Humans prefer some approaches over others.

### Preference Signal

Instead of rating absolute quality (1-10, which is subjective), we collect **relative preferences**:

```
âŒ Bad: "Edit A is 7/10"  (subjective, inconsistent between raters)
âœ… Good: "A is better than B"  (objective, consistent across raters)
```

Bradley-Terry model learns the pattern:
- If humans say "A is better than B" (strength 2), train model so:
  - `reward(A) - reward(B) â‰ˆ 2`

---

## Example: Full Workflow

### Scenario: You collected 50 preference pairs

**File: `feedback/preferences.json`**
```json
[
  {
    "song_id": "song_001",
    "edit_a_id": "temp_0.1",
    "edit_b_id": "temp_0.5",
    "preference": "b",
    "strength": 1,
    "reasoning": "Cut version is punchier"
  },
  {
    "song_id": "song_001",
    "edit_a_id": "temp_0.3",
    "edit_b_id": "temp_0.7",
    "preference": "a",
    "strength": 2,
    "reasoning": "0.3 keeps vocals better than 0.7"
  },
  ... (48 more pairs)
]
```

**Step 1: Train Reward Model**
```bash
$ python train_from_feedback.py --feedback feedback/preferences.json
```

Output:
```
âœ“ Loaded 50 preference pairs
  Train set: 45 pairs
  Val set: 5 pairs

TRAINING REWARD MODEL FROM PREFERENCES
Epoch  1/20 | Train Loss: 1.2341 | Val Loss: 1.1893
Epoch  2/20 | Train Loss: 0.8764 | Val Loss: 0.9234
Epoch  3/20 | Train Loss: 0.6234 | Val Loss: 0.7845
...
Epoch 20/20 | Train Loss: 0.2341 | Val Loss: 0.3123

Best epoch: 18
âœ“ Saved reward_model_v9_feedback_final.pt
```

**Step 2: Fine-tune Policy**
```bash
$ python train_rlhf_stable.py --episodes 500 --reward_model models/reward_model_v9_feedback_final.pt
```

Output:
```
RLHF TRAINING - SYNTHETIC DATA
Device: cuda
Episodes: 500

Episode  10/500 | Dense: 0.123 | Learned: 0.456 | Combined: 0.234 | Loss: 0.1234
Episode  20/500 | Dense: 0.145 | Learned: 0.523 | Combined: 0.267 | Loss: 0.1021
...
Episode 500/500 | Dense: 0.198 | Learned: 0.687 | Combined: 0.342 | Loss: 0.0456
  âœ“ New best: 0.342

âœ“ Training complete!
  Total episodes: 500
  Best combined reward: 0.342
```

**Step 3: Evaluate Improvement**

Generate edits with new policy and compare with old:
```
Old policy (v14): 45% win rate with new humans
New policy (v15): 67% win rate with new humans
Improvement: +22 percentage points! ğŸ‰
```

---

## What Happens Behind the Scenes

### Bradley-Terry Learning

The reward model learns through preference pairs:

```python
# If human says "Edit A is better (strength=2)"
preference_target = 2

# Train the model to output:
score_a = 5.2
score_b = 3.1
difference = 5.2 - 3.1 = 2.1  âœ“ Matches target!

# Loss = (2.1 - 2.0)Â² = 0.01 (small loss, good!)
```

After 50+ preferences, the model learns:
- "More beat alignment" â†’ higher score
- "Smoother transitions" â†’ higher score
- "Tighter edits" â†’ sometimes higher
- "Keeping vocals" â†’ important to humans

### Policy Gradient Update

The policy learns to chase the learned reward:

```python
# Policy generates action:
action = KEEP_BEAT

# Reward model scores it:
reward = 0.45  (human preferences say this is good!)

# Policy gradient:
gradient = âˆ‡ log Ï€(action | state) Ã— reward
update = policy_parameters - learning_rate Ã— gradient

# Policy learns: for this state, KEEP_BEAT is good!
```

Over 500 episodes, policy becomes expert at:
- Making edits humans like
- Choosing good beats to keep
- Creating smooth transitions
- Balancing tightness with musicality

---

## Success Metrics to Track

### After First Feedback Iteration

| Metric | Target | How to Measure |
|--------|--------|----------------|
| Feedback pairs collected | 50+ | Count annotations |
| Inter-rater agreement | 80%+ | Have 2+ people rate same songs |
| Reward model loss | <0.5 | Check training logs |
| Policy improvement | +10% | A/B test with new humans |

### After Second Iteration

| Metric | Target | How to Measure |
|--------|--------|----------------|
| Total feedback pairs | 200+ | Accumulate across iterations |
| Policy consistency | 85%+ | Hold-out test set |
| Human alignment | 70%+ | Does policy match human taste? |

---

## Tips for Success

### For Collecting Feedback

âœ“ **Take breaks** - Ear fatigue is real, take 5-min break every 30 mins
âœ“ **Be consistent** - If you rate song A as "tight and good", remember this for song B
âœ“ **Trust your instinct** - First impression is often best
âœ“ **Note-taking** - Write quick notes explaining choices (helps debugging later)
âœ“ **Multiple raters** - 2-3 people is better than 1 (catch biases)

### For Model Training

âœ“ **Start with confident feedback** - Only use annotations with confidence >0.7
âœ“ **Balance preferences** - Mix of "A better", "B better", "Tie" is healthy
âœ“ **Track convergence** - Plot training loss, watch for plateaus
âœ“ **Validate on held-out** - Don't train/test on same feedback

### For Policy Refinement

âœ“ **Warm-start** - Load policy_final.pt, don't start from scratch
âœ“ **Higher learning rate** - Fine-tuning, not pre-training
âœ“ **Monitor for overfitting** - Stop if policy learning diverges
âœ“ **Save frequently** - Save checkpoint every 50 episodes

---

## Troubleshooting

### "I have only 10 preference pairs - is that enough?"
- **No**, minimum ~30-50 for meaningful training
- **But**: Better than nothing! Train and observe
- **Next**: Collect 100+ pairs for reliable model

### "The new policy is worse than the old one"
- Could be reward model is learning wrong pattern
- Try: Lower learning rate, more epochs
- Or: Check feedback annotations for errors/contradictions

### "I keep changing my mind about preferences"
- Normal! Musical taste has ambiguity
- Solution: Get multiple people to rate same songs
- Average their preferences to reduce noise

### "Training is slow"
- Check GPU usage: `nvidia-smi`
- Reduce batch size if running out of memory
- Speed up: Fewer episodes (100 vs 500) for testing

---

## Next Steps After Feedback

### Option 1: Iterate (Recommended)
1. Collect 100+ more preference pairs
2. Train reward_model_v10
3. Fine-tune policy_v16
4. Measure improvement

### Option 2: Scale Up
1. Use annotation platform (Scale AI, Labelbox)
2. Collect 500+ pairs from diverse raters
3. Train production-quality models

### Option 3: Active Learning
1. Generate hard cases where models disagree
2. Collect focused feedback on disagreements
3. Resolve ambiguities
4. Converge faster

---

## Files You'll Create

```
feedback/
â”œâ”€ preferences.json                    (your annotations)
â”œâ”€ annotations_batch_001.csv          (optional CSV format)
â””â”€ quality_report.json                (optional validation results)

models/
â”œâ”€ reward_model_v9_feedback_best.pt    (from train_from_feedback.py)
â”œâ”€ reward_model_v9_feedback_final.pt   (best model to use)
â””â”€ policy_final.pt                     (updated from train_rlhf_stable.py)

eval_outputs/
â”œâ”€ evaluation_manifest.json            (from generate_eval_candidates.py)
â”œâ”€ song_001/
â”‚  â”œâ”€ song_001_temp_0.1.wav
â”‚  â”œâ”€ song_001_temp_0.3.wav
â”‚  â”œâ”€ song_001_temp_0.5.wav
â”‚  â”œâ”€ song_001_temp_0.7.wav
â”‚  â””â”€ song_001_temp_0.9.wav
â””â”€ song_002/
   â””â”€ ...
```

---

## You've Now Completed

âœ… Initial RLHF training (200 episodes)
âœ… Learned reward model (v8, synthetic data)
âœ… Basic policy (policy_final.pt)

## Next: Human Feedback Loop

Now you can:
1. Generate diverse edit candidates
2. Collect human preferences
3. Train model that aligns with humans
4. Improve policy quality

**Time estimate**: 2-5 hours for one feedback iteration
**Expected result**: 15-25% improvement in policy quality

Ready to start? â†’ Run `python generate_eval_candidates.py` ğŸš€
